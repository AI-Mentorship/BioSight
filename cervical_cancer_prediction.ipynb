{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Create validation/test datasets***",
   "id": "30ec5c5389fe9e0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:48:17.417745Z",
     "start_time": "2025-10-10T04:48:17.415107Z"
    }
   },
   "cell_type": "code",
   "source": "import os, random, shutil",
   "id": "c1455968dbb74d00",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-10T04:57:24.419983Z",
     "start_time": "2025-10-10T04:57:14.568706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Copy the original data into test, train, and validate folders\n",
    "\n",
    "path_datasets = 'datasets/original_data'\n",
    "train_dataset = './datasets/train'\n",
    "test_dataset = './datasets/test'\n",
    "validate_dataset = './datasets/validate'\n",
    "\n",
    "split= 0.7\n",
    "\n",
    "data_dirs = [\n",
    "    'cervix_dyk',\n",
    "    'cervix_koc',\n",
    "    'cervix_mep',\n",
    "    'cervix_pab',\n",
    "    'cervix_sfi',\n",
    "]\n",
    "\n",
    "for cervix_ds in data_dirs:\n",
    "    # Check to see if files have already been copied\n",
    "    if len(os.listdir(train_dataset)) == 5:\n",
    "        break\n",
    "\n",
    "    path_cervix_ds = os.path.join(path_datasets, cervix_ds)\n",
    "    files = [\n",
    "        f for f in os.listdir(path_cervix_ds)\n",
    "        if os.path.isfile(os.path.join(path_cervix_ds, f))\n",
    "    ]\n",
    "\n",
    "    num_sample = int(len(files) * split)\n",
    "    training_files = random.sample(files, num_sample)\n",
    "\n",
    "    for source_file in training_files:\n",
    "        source_file_path = os.path.join(path_cervix_ds, source_file)\n",
    "        destination_dir = os.path.join(train_dataset, cervix_ds)\n",
    "\n",
    "        if not os.path.exists(destination_dir):\n",
    "            os.makedirs(destination_dir)\n",
    "\n",
    "        shutil.copy2(source_file_path, destination_dir)\n",
    "\n",
    "    files = [\n",
    "        f for f in os.listdir(path_cervix_ds)\n",
    "        if f not in training_files\n",
    "    ]\n",
    "\n",
    "    num_sample = int(len(files) * 0.5)\n",
    "    testing_files = random.sample(files, num_sample)\n",
    "\n",
    "    for source_file in testing_files:\n",
    "        source_file_path = os.path.join(path_cervix_ds, source_file)\n",
    "        destination_dir = os.path.join(test_dataset, cervix_ds)\n",
    "\n",
    "        if not os.path.exists(destination_dir):\n",
    "            os.makedirs(destination_dir)\n",
    "\n",
    "        shutil.copy2(source_file_path, destination_dir)\n",
    "\n",
    "    validation_files = [\n",
    "        f for f in os.listdir(path_cervix_ds)\n",
    "        if f not in training_files and f not in testing_files\n",
    "    ]\n",
    "\n",
    "    for source_file in validation_files:\n",
    "        source_file_path = os.path.join(path_cervix_ds, source_file)\n",
    "        destination_dir = os.path.join(validate_dataset, cervix_ds)\n",
    "\n",
    "        if not os.path.exists(destination_dir):\n",
    "            os.makedirs(destination_dir)\n",
    "\n",
    "        shutil.copy2(source_file_path, destination_dir)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:57:10.983208Z",
     "start_time": "2025-10-10T04:57:10.979469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Destroy all training, testing, and validation data\n",
    "\n",
    "shutil.rmtree(train_dataset)\n",
    "shutil.rmtree(test_dataset)\n",
    "shutil.rmtree(validate_dataset)\n",
    "\n",
    "os.makedirs(train_dataset)\n",
    "os.makedirs(test_dataset)\n",
    "os.makedirs(validate_dataset)"
   ],
   "id": "4c9120f3e360d969",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Preprocess and Label data***",
   "id": "5adaa863252b0545"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:58:52.252312Z",
     "start_time": "2025-10-10T04:58:52.249060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "id": "ad7ad1f8a40f5bc3",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:59:21.994395Z",
     "start_time": "2025-10-10T04:58:52.890159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cancer_type = ['cervix_dyk', 'cervix_koc', 'cervix_mep', 'cervix_pab', 'cervix_sfi']\n",
    "\n",
    "train_dataset_paths = [\n",
    "    os.path.join(train_dataset, ct) for ct in cancer_type\n",
    "]\n",
    "\n",
    "test_dataset_paths = [\n",
    "    os.path.join(test_dataset, ct) for ct in cancer_type\n",
    "]\n",
    "\n",
    "validation_dataset_paths = [\n",
    "    os.path.join(validate_dataset, ct) for ct in cancer_type\n",
    "]\n",
    "\n",
    "# Training data before preprocessing\n",
    "i = 0\n",
    "data_bptr = []\n",
    "\n",
    "for train_dp in train_dataset_paths:\n",
    "    for img_file in os.listdir(train_dp):\n",
    "        img = cv2.imread(os.path.join(train_dp, img_file))\n",
    "        data_bptr.append((img, i))\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "# Test data before preprocessing\n",
    "i = 0\n",
    "data_bpt = []\n",
    "\n",
    "for test_dp in test_dataset_paths:\n",
    "    for img_file in os.listdir(test_dp):\n",
    "        img = cv2.imread(os.path.join(test_dp, img_file))\n",
    "        data_bpt.append((img,i))\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "# Validation data before preprocessing\n",
    "i = 0\n",
    "data_bpv = []\n",
    "\n",
    "for val_dp in validation_dataset_paths:\n",
    "    for img_file in os.listdir(val_dp):\n",
    "        img = cv2.imread(os.path.join(val_dp, img_file))\n",
    "        data_bpv.append((img,i))\n",
    "\n",
    "    i = i + 1"
   ],
   "id": "aaa2bb05c2d9326d",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:59:24.319275Z",
     "start_time": "2025-10-10T04:59:24.313350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Classes for training, test, and validation data. This is so that we can use useful features\n",
    "#   (e.g., batching, shuffling) and to easily apply transformations on data\n",
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        elm = self.transform(self.data[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return elm, label"
   ],
   "id": "758e16738f8bc452",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:59:27.730659Z",
     "start_time": "2025-10-10T04:59:27.719074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessed data of Dataset type\n",
    "\n",
    "# Unpacking each pair of images and labels\n",
    "# [(img,label),...] --> ((img,...), (label,...)) --> (img,...) and (label,...)\n",
    "training_data = CancerDataset(*zip(*data_bptr))\n",
    "test_data = CancerDataset(*zip(*data_bpt))\n",
    "validation_data = CancerDataset(*zip(*data_bpv))"
   ],
   "id": "1cf5af6535e604cf",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Dataloaders***",
   "id": "589b2e09b700cde9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:59:31.151049Z",
     "start_time": "2025-10-10T04:59:31.145449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loads 8 training data randomly from training_data\n",
    "# batch_size of 8 is chosen since image size is 3x512x512 which will be computationally expensive and\n",
    "#   some GPUs wouldn't be able to handle that much load\n",
    "training_dataloader = DataLoader(training_data, batch_size=8, shuffle=True)\n",
    "\n",
    "# There's no need to shuffle the test and validation data\n",
    "test_dataloader = DataLoader(test_data, batch_size=8)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=8)"
   ],
   "id": "d516368166659bd6",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***CNN Model Building***",
   "id": "cc0dbc4df2b94d70"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:59:33.144721Z",
     "start_time": "2025-10-10T04:59:33.141886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f"
   ],
   "id": "a83c4bb3126af1b8",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:59:34.956960Z",
     "start_time": "2025-10-10T04:59:34.560567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1st layer - in: RGB (3 channels), out: 32 channels\n",
    "        #   ---> 5th layer - in: 215 channels, out: 512 channels\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        # Max pooling and this halves the input size\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layer: output features connected to all input features\n",
    "        # Input features: 512*16*16 ---> Output features: 5\n",
    "        self.fc1 = nn.Linear(in_features=512*16*16, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.fc3 = nn.Linear(in_features=512, out_features=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pool after every convolution\n",
    "        x = self.pool(f.relu(self.conv1(x)))\n",
    "        x = self.pool(f.relu(self.conv2(x)))\n",
    "        x = self.pool(f.relu(self.conv3(x)))\n",
    "        x = self.pool(f.relu(self.conv4(x)))\n",
    "        x = self.pool(f.relu(self.conv5(x)))\n",
    "\n",
    "        # Flatten output\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = f.relu(self.fc1(x))\n",
    "        x = f.relu(self.fc2(x))\n",
    "        x = f.relu(self.fc3(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "my_model = CNN()"
   ],
   "id": "dd3afb3d79b71136",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Importing Model***",
   "id": "a6d42b1baeb1cf11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:59:37.219111Z",
     "start_time": "2025-10-10T04:59:37.215791Z"
    }
   },
   "cell_type": "code",
   "source": "import torchvision.models as models",
   "id": "42b2b53a86152681",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:59:38.722987Z",
     "start_time": "2025-10-10T04:59:38.412547Z"
    }
   },
   "cell_type": "code",
   "source": "resnet50_cdt = models.resnet50()",
   "id": "c665ce715ab69b71",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Train Model***",
   "id": "8cfa44a067bf4867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T04:59:40.863044Z",
     "start_time": "2025-10-10T04:59:40.860220Z"
    }
   },
   "cell_type": "code",
   "source": "from tqdm import tqdm",
   "id": "4d62c21cab41c9fd",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T05:02:19.424329Z",
     "start_time": "2025-10-10T05:02:19.413041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss function for multiclass classification\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# Standard adam optimizer with learning rate of 1e-4\n",
    "optimizer = torch.optim.Adam(resnet50_cdt.parameters(), lr=1e-4)\n",
    "#optimizer = torch.optim.Adam(my_model.parameters(), lr=1e-4)\n",
    "# Set device as gpu if it exists\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train(num_epochs, model_type):\n",
    "    model_type.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch [{}/{}]\".format(epoch + 1,num_epochs))\n",
    "\n",
    "        # Set model to training mode\n",
    "        model_type.train()\n",
    "        for (data, labels) in tqdm(training_dataloader):\n",
    "            # If cuda exists, data will be moved to the GPU memory\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model_type(data)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(outputs, labels)\n",
    "            # Set gradient to 0 before backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()"
   ],
   "id": "f6728555a9265480",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T05:02:40.861664Z",
     "start_time": "2025-10-10T05:02:22.669504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "train(EPOCHS, resnet50_cdt)\n",
    "#train(EPOCHS, my_model)"
   ],
   "id": "5cc7e83fc1443035",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/2188 [00:17<1:33:14,  2.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[44]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m EPOCHS = \u001B[32m1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresnet50_cdt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m#train(EPOCHS, my_model)\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 27\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(num_epochs, model_type)\u001B[39m\n\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# Set gradient to 0 before backpropagation\u001B[39;00m\n\u001B[32m     26\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[38;5;66;03m# Update weights\u001B[39;00m\n\u001B[32m     29\u001B[39m optimizer.step()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work/ML_Projects/.venv/lib/python3.12/site-packages/torch/_tensor.py:647\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    637\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    638\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    639\u001B[39m         Tensor.backward,\n\u001B[32m    640\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    645\u001B[39m         inputs=inputs,\n\u001B[32m    646\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m647\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    648\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work/ML_Projects/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Work/ML_Projects/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    827\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    828\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m829\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    830\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    831\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    832\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    833\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Evaluate Model***",
   "id": "3bce70bf1f17c2b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T05:02:45.837713Z",
     "start_time": "2025-10-10T05:02:45.329121Z"
    }
   },
   "cell_type": "code",
   "source": "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassAccuracy",
   "id": "d68578f14e16d53",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T05:02:47.624425Z",
     "start_time": "2025-10-10T05:02:47.620686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model_type):\n",
    "    # Set model to evaluation mode\n",
    "    model_type.eval()\n",
    "\n",
    "    # Precision: How many were positive (correctly identifying cancer type) out of all predicted positives\n",
    "    # Recall: How many were correctly positive out of all positives\n",
    "    # Accuracy: How many did the model correctly predict\n",
    "    precision_metric = MulticlassPrecision(num_classes=5)\n",
    "    recall_metric = MulticlassRecall(num_classes=5)\n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=5)\n",
    "\n",
    "    # Disable gradient tracking\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            outputs = model_type(inputs)\n",
    "\n",
    "            # Calculate precision, recall, and accuracy as we iterate through the test dataloader\n",
    "            precision_metric.update(outputs, labels)\n",
    "            recall_metric.update(outputs, labels)\n",
    "            accuracy_metric.update(outputs, labels)\n",
    "\n",
    "    # Compute final precision, recall, and accuracy\n",
    "    precision = precision_metric.compute()\n",
    "    recall = recall_metric.compute()\n",
    "    accuracy = accuracy_metric.compute()\n",
    "    print(\"Precision: {}, Recall: {}, Accuracy: {}\".format(i + 1, precision, recall, accuracy))"
   ],
   "id": "f5eb4d68ea911878",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "evaluate(resnet50_cdt)\n",
    "#evaluate(my_model)"
   ],
   "id": "2e960c87db89749c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Saving Model***",
   "id": "d5085b71b34cf14a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(resnet50_cdt.state_dict(), \"./resnet50_cdt.pth\")",
   "id": "470755381a3a7c7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***Loading Model***",
   "id": "9ba229b87e540399"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T06:15:56.887624Z",
     "start_time": "2025-10-09T06:15:56.576080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load resnet50 without weights\n",
    "test_model = models.resnet50(weights=None)\n",
    "# Load weights into the model\n",
    "test_model.load_state_dict(torch.load(\"./resnet50_cdt.pth\"))\n",
    "test_model.eval()"
   ],
   "id": "b9c67091309aaec5",
   "outputs": [],
   "execution_count": 59
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
