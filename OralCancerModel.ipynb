{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab5b987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishpatel/Desktop/UTD/Projects/ProjectAIS/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5e6bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/krishpatel/.cache/kagglehub/datasets/obulisainaren/multi-cancer/versions/3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = \"/content/kagglehub_data/obulisainaren/multi-cancer\"\n",
    "\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    shutil.rmtree(dataset_path)\n",
    "\n",
    "path = kagglehub.dataset_download(\"obulisainaren/multi-cancer\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d4c708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oral_normal': 0, 'oral_scc': 1}\n",
      "Classes: ['oral_normal', 'oral_scc']\n",
      "Number of images: 10002\n",
      "Class to index mapping: {'oral_normal': 0, 'oral_scc': 1}\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "oral_cancer_dir = os.path.join(path, \"Multi Cancer\", \"Multi Cancer\", \"Oral Cancer\")\n",
    "\n",
    "oral_dataset = datasets.ImageFolder(root=oral_cancer_dir, transform=transform)\n",
    "print(oral_dataset.class_to_idx)\n",
    "\n",
    "\n",
    "oral_dataset = datasets.ImageFolder(root=oral_cancer_dir, transform=transform)\n",
    "\n",
    "\n",
    "oral_loader = DataLoader(oral_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"Classes:\", oral_dataset.classes)\n",
    "print(\"Number of images:\", len(oral_dataset))\n",
    "print(\"Class to index mapping:\", oral_dataset.class_to_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395f78be",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(oral_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(\n",
    "    oral_dataset, [train_size, test_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a936bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa00860",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OralCancerClassifier(nn.Module):\n",
    "  def __init__(self, num_classes=2):\n",
    "      super(OralCancerClassifier, self).__init__()\n",
    "      self.base_model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "      self.features = nn.Sequential(*list(self.base_model.children())[:-1])\n",
    "      enet_out_size = 1280\n",
    "      self.classifier = nn.Linear(enet_out_size, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.features(x)\n",
    "      x = torch.flatten(x, 1)\n",
    "      output = self.classifier(x)\n",
    "      return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05fa08ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OralCancerClassifier(num_classes=2).to(device)\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fee0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aadae34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f4391cf86e473d879a9213d98d1853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n",
      "\u001b[1;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;32m     15\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\n",
      "File \u001b[0;32m~/Desktop/UTD/Projects/ProjectAIS/venv/lib/python3.9/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n",
      "\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n",
      "\u001b[1;32m    646\u001b[0m     )\n",
      "\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n",
      "\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/UTD/Projects/ProjectAIS/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n",
      "\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/UTD/Projects/ProjectAIS/venv/lib/python3.9/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n",
      "\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce56def",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy: {100*correct/total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIS Kernel",
   "language": "python",
   "name": "aispro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
